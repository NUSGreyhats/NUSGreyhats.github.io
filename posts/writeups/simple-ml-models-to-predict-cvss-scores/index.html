<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Simple ML models to predict CVSS scores | NUS Greyhats</title>
<meta name=keywords content="writeups,cyberAI">
<meta name=description content="An simple exploration on predicting CVSS scores using CVE descriptions">
<meta name=author content="Uxinnn">
<link rel=canonical href=https://nusgreyhats.org/posts/writeups/simple-ml-models-to-predict-cvss-scores/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link rel=preconnect href=https://fonts.googleapis.com>
<link rel=preconnect href=https://fonts.gstatic.com crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&display=swap" rel=stylesheet>
<link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;500&family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&display=swap" rel=stylesheet>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.b268fb3bd1950526ba32df2898e8dfc3171f0fecdf09cd6945557b277f0eecf8.css integrity="sha256-smj7O9GVBSa6Mt8omOjfwxcfD+zfCc1pRVV7J38O7Pg=" rel="preload stylesheet" as=style>
<link rel=preload href=/greyhats-dark.png as=image>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nusgreyhats.org/greyhats.png>
<link rel=icon type=image/png sizes=16x16 href=https://nusgreyhats.org/greyhats.png>
<link rel=icon type=image/png sizes=32x32 href=https://nusgreyhats.org/greyhats.png>
<link rel=apple-touch-icon href=https://nusgreyhats.org/greyhats.png>
<link rel=mask-icon href=https://nusgreyhats.org/greyhats.png>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="Simple ML models to predict CVSS scores">
<meta property="og:description" content="An simple exploration on predicting CVSS scores using CVE descriptions">
<meta property="og:type" content="article">
<meta property="og:url" content="https://nusgreyhats.org/posts/writeups/simple-ml-models-to-predict-cvss-scores/"><meta property="og:image" content="https://nusgreyhats.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2022-01-14T00:00:00+00:00">
<meta property="article:modified_time" content="2022-01-14T00:00:00+00:00"><meta property="og:site_name" content="NUS Greyhats">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://nusgreyhats.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="Simple ML models to predict CVSS scores">
<meta name=twitter:description content="An simple exploration on predicting CVSS scores using CVE descriptions">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nusgreyhats.org/posts/"},{"@type":"ListItem","position":2,"name":"Simple ML models to predict CVSS scores","item":"https://nusgreyhats.org/posts/writeups/simple-ml-models-to-predict-cvss-scores/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Simple ML models to predict CVSS scores","name":"Simple ML models to predict CVSS scores","description":"An simple exploration on predicting CVSS scores using CVE descriptions","keywords":["writeups","cyberAI"],"articleBody":"Overview This writeup details the construction of rudimentary machine learning models that use the descriptions of CVEs to predict their corresponding base CVSS scores. A brief illustration is shown below.\nThe original idea originates from an internship that I have done, and this personal project is an attempt on my own to improve and expand on what I done previously. The project is still a work in progress.\nPython and its associated libraries (mainly Pandas, Sklearn and some SpaCy) are used.\nRaw Data Gathering and Filtering Data for this project is taken from CVE JSON data feeds provided by the National Vulnerability Database (NVD). Each data feed contains details of CVEs in a particular year, with the earliest data feed coming from the year 2002. The JSON data feeds contain tons of data about each CVE, such as the CWEs associated with it, references, descriptions, CVSS V2 and V3 metrics, and published dates.\nAs the JSON data feeds are quite complex, they cannot be easily parsed into a dataframe using Pandas and Python’s json library due to their extremely nested structure and missing fields when data is absent. Thus, I decided to just extract the essential data (CVE description, base CVSS V2 score, and base CVSS V3 score) that will be needed from the data feeds instead of trying to parse all data into a nice dataframe. Since not every CVE has an associated CVSS V2 or V3 score, missing scores are temporarily replaced with a None when extracting these data. The function used is shown here:\ndef get_useful_features(raw_cve_entry): entry = dict() try: entry[\"description\"] = raw_cve_entry[\"cve\"][\"description\"][\"description_data\"][0][\"value\"] except KeyError: entry[\"description\"] = None try: entry[\"baseScoreV3\"] = raw_cve_entry[\"impact\"][\"baseMetricV3\"][\"cvssV3\"][\"baseScore\"] except KeyError: entry[\"baseScoreV3\"] = None try: entry[\"baseScoreV2\"] = raw_cve_entry[\"impact\"][\"baseMetricV2\"][\"cvssV2\"][\"baseScore\"] except KeyError: entry[\"baseScoreV2\"] = None return entry The extracted essential data can be seen below.\nAfter extracting the data, a quick look at the data will show that there are many unusable CVEs that are included in the data (See row 177474 in the figure above). The different reasons for the CVEs to be not usable is shown below, along with the number of CVEs that is tagged with that reason.\n   Reason for not being used Number of CVEs     REJECT 10349   DISPUTED 897   UNSUPPORTED WHEN ASSIGNED 91   PRODUCT NOT SUPPORTED WHEN ASSIGNED 6   UNVERIFIABLE 5   UNVERIFIABLE, PRERELEASE 2   SPLIT 1    Since they all had a description that starts with **  **, a quick regex matching was done to filter out and remove these unneeded data.\ndf = df[~(df.description.str.contains('^\\\\*\\\\*\\\\s+[A-Z]+\\\\s+\\\\*\\\\*\\\\s+'))] # Remove unneeded CVEs Initially, I thought of using both base CVSS V2 and V3 scores in the project, simply to see which score can be better predicted. However, after looking at the counts of each metric, I realised that the number of CVEs with base CVSS V2 scores (165904) vastly outnumbered the number of CVEs with base CVSS V3 scores (92983). As such, I then decided to remove all CVEs that do not have a base CVSS V2 score and just focus on predicting base CVSS V2 scores, simply as that would allow me more data to work with.\ndf.dropna(inplace=True, subset=[\"baseScoreV2\"]) After the removal of unusable CVEs and CVEs that do not have an associated base CVSS V2 score, I was left with a total of 165904 rows of CVE entries, shown below.\nThe CVE entries are then split into train and test data.\nX = df[\"description\"] y = df[\"baseScoreV2\"] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) The distribution of base CVSS V2 scores of the training data is shown below.\nML Pipeline Construction Since I have filtered out all unneedded data, I can proceed to create the ML pipelines. The pipelines will consists of 3 steps:\n Vectorizer to convert CVE descriptions into a vector form suitable to be fed into ML algorithms. An optional PCA algorithm to reduce the dimensions of the input from the vectorizer. The actual ML model that will be used.  NLP Processing and Vectorizing Firstly, I chose the easy way out and used the Bag-of-Words (BoW) and TF-IDF models to vectorize the descriptions. These 2 models can be invoked by simply using Sklearn’s CountVectorizer and TfidfVectorizer. To augment these vectorizers, I used SpaCy to write a custom tokenizer function to do basic NLP processing on these descriptions which can be fed into Sklearn’s vectorizers. The Python library SpaCy is used for this purpose since it contains pretrained NLP pipelines which simplifies this processing step immensely.\n# Create custom tokenizer def tokenizer(desc): tokens = nlp(desc) tokens = [word.lemma_.lower().strip() for word in tokens] tokens = [word for word in tokens if word not in nlp.Defaults.stop_words and not set(word).issubset(punctuation)] return tokens The function first feeds each description into SpaCy’s en_core_web_lg pretrained pipeline, where they are tokenized. Then, each token is stripped off leading and trailing whitespaces, before being converted to lowercase and lemmatized. If the token is a punctuation or a stop word, it is removed too. The result is then a list of tokens that will be used by Sklearn’s vectorizers to fit the BoW and TF-IDF models. The output of the vectorizers will then be a word vector that represents the CVE description that was fed in.\nThe vectorizers and parameters used are shown below:\nCountVectorizer(tokenizer=tokenizer, ngram_range=(1,2), max_df=0.5, min_df=5, max_features=10000,) TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1,2), max_df=0.5, min_df=5, max_features=10000,) PCA for Dimensionality Reduction As the dimensions of the word vectors are very large due to the large vocabulary accumulated from all the training CVE descriptions, PCA is used reduce the number of dimensions of the word vectors. However, as including this step increases the computational time by a lot, this step is ommitted in most of the pipelines that I created.\nTruncatedSVD(n_components=10000, n_iter=7, random_state=42) Choosing ML Models Since my aim is just to explore which ML model will give a better result, I chose a few simple models to get a basic feel of how well a model can predict the base CVSS V2 scores using their associated CVE descriptions (The models were also chosen with considerations for my potato laptop).\nModels chosen were:\n Linear Regression KNN Decision Trees Gradient Boosting Regression (Xgboost)  With the exception of Linear Regression which was fitted on transformed features from both BoW and TF-IDF vectorizers, the other 3 models were only fitted on transformed data from the TF-IDF vectorizer.\nlinear_regr = LinearRegression() knn_regr = KNeighborsRegressor() dt_regr = DecisionTreeRegressor(min_samples_split=5, min_samples_leaf=3,) xgboost_regr = GradientBoostingRegressor() Results To score results of the models, Mean Square Error was chosen as the evaluation metric, since I want to penalize models more for larger errors. As the target base CVSS V2 score only ranges from 0 to 10, the predicted values of the models are further fed into a function that constrains the predicted values to this range (predicted values that are less than 0 to 0, and all predicted values more than 10 to 10).\ny_pred = np.clip(pipeline.predict(X_test), 0, 10) mse = mean_squared_error(y_test, y_pred) Results are shown in the table below. Do note fine tuning of models have not been done as of the writing of this writeup since it is probably going to take a long time to run and burn up my laptop.\n   Model Vectorizer Have PCA MSE     Linear Regression BoW False 1.62   Linear Regression TF-IDF False 1.66   Linear Regression BoW True 1.59   KNN TF-IDF False 1.92   Decision Trees TF-IDF False 2.22   Xgboost TF-IDF False 1.91    Discussion and Future Work As can be seen, the BoW vectorizer seem to produce slightly better results than the TF-IDF vectorizer. Inclusion of the PCA algorithm also provides some slight improvements. Comparing the different models used, linear regression seem to have produced the best results.\nOverall, the best MSE of 1.59 is a good sign, especially since the pipelines are not optimised at all. This shows that the ML model will on average have an error of about 1.59 when prediciting CVSS scores, which is still acceptable since 1.59 is not too much over the range of 0 to 10.\nIn the future, work will be done in the following aspects:\n Constructing a recurrent neural network to compare against the performance of these classic ML algorithms. Perform more pre-processing on the CVSS scores, since they are not very normally distributed. Incorporate word vectors and compare performance against the BoW and TF-IDF vectorizers. Optimising hyperparameters to get an optimal model. Employing the model for various use cases.  References  https://nvd.nist.gov/vuln/data-feeds# https://scikit-learn.org/stable/modules/classes.html https://pandas.pydata.org/docs/ https://spacy.io/usage/linguistic-features https://www.kaggle.com/nkitgupta/text-representations https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle https://www.ibm.com/support/pages/transforming-variable-normality-parametric-statistics https://towardsdatascience.com/working-with-sparse-data-sets-in-pandas-and-sklearn-d26c1cfbe067  ","wordCount":"1388","inLanguage":"en","datePublished":"2022-01-14T00:00:00Z","dateModified":"2022-01-14T00:00:00Z","author":{"@type":"Person","name":"Uxinnn"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nusgreyhats.org/posts/writeups/simple-ml-models-to-predict-cvss-scores/"},"publisher":{"@type":"Organization","name":"NUS Greyhats","logo":{"@type":"ImageObject","url":"https://nusgreyhats.org/greyhats.png"}}}</script>
</head>
<body class=dark id=top>
<script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://nusgreyhats.org/ accesskey=h title="NUS Greyhats (Alt + H)">
<img src=/greyhats-dark.png alt=logo aria-label=logo id=logo height=35>
<script>localStorage.getItem("pref-theme")==="light"?document.getElementById("logo").src="/greyhats.png":document.getElementById("logo").src="/greyhats-dark.png"</script>NUS Greyhats</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://nusgreyhats.org/team title="the team">
<span>the team</span>
</a>
</li>
<li>
<a href=https://nusgreyhats.org/secweds title=secweds>
<span>secweds</span>
</a>
</li>
<li>
<a href=https://nusgreyhats.org/resources/ title=resources>
<span>resources</span>
</a>
</li>
<li>
<a href=https://nusgreyhats.org/tags/ title=tags>
<span>tags</span>
</a>
</li>
<li>
<a href=https://nusgreyhats.org/search/ title="search (Alt + /)" accesskey=/>
<span>search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Simple ML models to predict CVSS scores
</h1>
<div class=post-description>
An simple exploration on predicting CVSS scores using CVE descriptions
</div>
<div class=post-meta>January 14, 2022&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Uxinnn&nbsp;|&nbsp;<a href=https://github.com/NUSGreyhats/NUSGreyhats.github.io/tree/master/content/posts/writeups/Simple%20ML%20models%20to%20predict%20CVSS%20scores.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header>
<div class=post-content><h1 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h1>
<p>This writeup details the construction of rudimentary machine learning models that use the descriptions of CVEs to predict their corresponding base CVSS scores.
A brief illustration is shown below.</p>
<p><img loading=lazy src=/images/cvss-prediction/model-overview.png alt>
</p>
<p>The original idea originates from an internship that I have done, and this personal project is an attempt on my own to improve and expand on what I done previously.
The project is still a work in progress.</p>
<p>Python and its associated libraries (mainly Pandas, Sklearn and some SpaCy) are used.</p>
<h1 id=raw-data-gathering-and-filtering>Raw Data Gathering and Filtering<a hidden class=anchor aria-hidden=true href=#raw-data-gathering-and-filtering>#</a></h1>
<p>Data for this project is taken from <a href=https://nvd.nist.gov/vuln/data-feeds#>CVE JSON data feeds</a> provided by the National Vulnerability Database (NVD).
Each data feed contains details of CVEs in a particular year, with the earliest data feed coming from the year 2002.
The JSON data feeds contain tons of data about each CVE, such as the CWEs associated with it, references, descriptions, CVSS V2 and V3 metrics, and published dates.</p>
<p>As the JSON data feeds are quite complex, they cannot be easily parsed into a dataframe using Pandas and Python&rsquo;s json library due to their extremely nested structure and missing fields when data is absent.
Thus, I decided to just extract the essential data (CVE description, base CVSS V2 score, and base CVSS V3 score) that will be needed from the data feeds instead of trying to parse all data into a nice dataframe.
Since not every CVE has an associated CVSS V2 or V3 score, missing scores are temporarily replaced with a <code>None</code> when extracting these data.
The function used is shown here:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_useful_features</span>(raw_cve_entry):
    entry <span style=color:#f92672>=</span> dict()
    <span style=color:#66d9ef>try</span>:
        entry[<span style=color:#e6db74>&#34;description&#34;</span>] <span style=color:#f92672>=</span> raw_cve_entry[<span style=color:#e6db74>&#34;cve&#34;</span>][<span style=color:#e6db74>&#34;description&#34;</span>][<span style=color:#e6db74>&#34;description_data&#34;</span>][<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;value&#34;</span>]
    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>KeyError</span>:
        entry[<span style=color:#e6db74>&#34;description&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
    <span style=color:#66d9ef>try</span>:
        entry[<span style=color:#e6db74>&#34;baseScoreV3&#34;</span>] <span style=color:#f92672>=</span> raw_cve_entry[<span style=color:#e6db74>&#34;impact&#34;</span>][<span style=color:#e6db74>&#34;baseMetricV3&#34;</span>][<span style=color:#e6db74>&#34;cvssV3&#34;</span>][<span style=color:#e6db74>&#34;baseScore&#34;</span>]
    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>KeyError</span>:
        entry[<span style=color:#e6db74>&#34;baseScoreV3&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
    <span style=color:#66d9ef>try</span>:
        entry[<span style=color:#e6db74>&#34;baseScoreV2&#34;</span>] <span style=color:#f92672>=</span> raw_cve_entry[<span style=color:#e6db74>&#34;impact&#34;</span>][<span style=color:#e6db74>&#34;baseMetricV2&#34;</span>][<span style=color:#e6db74>&#34;cvssV2&#34;</span>][<span style=color:#e6db74>&#34;baseScore&#34;</span>]
    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>KeyError</span>:
        entry[<span style=color:#e6db74>&#34;baseScoreV2&#34;</span>] <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
    <span style=color:#66d9ef>return</span> entry
</code></pre></div><p>The extracted essential data can be seen below.</p>
<p><img loading=lazy src=/images/cvss-prediction/data-after-extraction.png alt>
</p>
<p>After extracting the data, a quick look at the data will show that there are many unusable CVEs that are included in the data (See row 177474 in the figure above).
The different reasons for the CVEs to be not usable is shown below, along with the number of CVEs that is tagged with that reason.</p>
<table>
<thead>
<tr>
<th style=text-align:left>Reason for not being used</th>
<th style=text-align:left>Number of CVEs</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left>REJECT</td>
<td style=text-align:left>10349</td>
</tr>
<tr>
<td style=text-align:left>DISPUTED</td>
<td style=text-align:left>897</td>
</tr>
<tr>
<td style=text-align:left>UNSUPPORTED WHEN ASSIGNED</td>
<td style=text-align:left>91</td>
</tr>
<tr>
<td style=text-align:left>PRODUCT NOT SUPPORTED WHEN ASSIGNED</td>
<td style=text-align:left>6</td>
</tr>
<tr>
<td style=text-align:left>UNVERIFIABLE</td>
<td style=text-align:left>5</td>
</tr>
<tr>
<td style=text-align:left>UNVERIFIABLE, PRERELEASE</td>
<td style=text-align:left>2</td>
</tr>
<tr>
<td style=text-align:left>SPLIT</td>
<td style=text-align:left>1</td>
</tr>
</tbody>
</table>
<p>Since they all had a description that starts with <code>** &lt;REASON> **</code>, a quick regex matching was done to filter out and remove these unneeded data.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>df <span style=color:#f92672>=</span> df[<span style=color:#f92672>~</span>(df<span style=color:#f92672>.</span>description<span style=color:#f92672>.</span>str<span style=color:#f92672>.</span>contains(<span style=color:#e6db74>&#39;^</span><span style=color:#ae81ff>\\</span><span style=color:#e6db74>*</span><span style=color:#ae81ff>\\</span><span style=color:#e6db74>*</span><span style=color:#ae81ff>\\</span><span style=color:#e6db74>s+[A-Z]+</span><span style=color:#ae81ff>\\</span><span style=color:#e6db74>s+</span><span style=color:#ae81ff>\\</span><span style=color:#e6db74>*</span><span style=color:#ae81ff>\\</span><span style=color:#e6db74>*</span><span style=color:#ae81ff>\\</span><span style=color:#e6db74>s+&#39;</span>))]  <span style=color:#75715e># Remove unneeded CVEs</span>
</code></pre></div><p>Initially, I thought of using both base CVSS V2 and V3 scores in the project, simply to see which score can be better predicted.
However, after looking at the counts of each metric, I realised that the number of CVEs with base CVSS V2 scores (165904) vastly outnumbered the number of CVEs with base CVSS V3 scores (92983).
As such, I then decided to remove all CVEs that do not have a base CVSS V2 score and just focus on predicting base CVSS V2 scores, simply as that would allow me more data to work with.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>df<span style=color:#f92672>.</span>dropna(inplace<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, subset<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;baseScoreV2&#34;</span>])
</code></pre></div><p>After the removal of unusable CVEs and CVEs that do not have an associated base CVSS V2 score, I was left with a total of 165904 rows of CVE entries, shown below.</p>
<p><img loading=lazy src=/images/cvss-prediction/data-after-filtering.png alt>
</p>
<p>The CVE entries are then split into train and test data.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>X <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#34;description&#34;</span>]
y <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#34;baseScoreV2&#34;</span>]

X_train, X_test, y_train, y_test <span style=color:#f92672>=</span> train_test_split(X, y, test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>)
</code></pre></div><p>The distribution of base CVSS V2 scores of the training data is shown below.</p>
<p><img loading=lazy src=/images/cvss-prediction/cvss-score-distribution.png alt>
</p>
<h1 id=ml-pipeline-construction>ML Pipeline Construction<a hidden class=anchor aria-hidden=true href=#ml-pipeline-construction>#</a></h1>
<p>Since I have filtered out all unneedded data, I can proceed to create the ML pipelines.
The pipelines will consists of 3 steps:</p>
<ol>
<li>Vectorizer to convert CVE descriptions into a vector form suitable to be fed into ML algorithms.</li>
<li>An optional PCA algorithm to reduce the dimensions of the input from the vectorizer.</li>
<li>The actual ML model that will be used.</li>
</ol>
<h2 id=nlp-processing-and-vectorizing>NLP Processing and Vectorizing<a hidden class=anchor aria-hidden=true href=#nlp-processing-and-vectorizing>#</a></h2>
<p>Firstly, I chose the easy way out and used the Bag-of-Words (BoW) and TF-IDF models to vectorize the descriptions. These 2 models can be invoked by simply using Sklearn&rsquo;s CountVectorizer and TfidfVectorizer.
To augment these vectorizers, I used SpaCy to write a custom tokenizer function to do basic NLP processing on these descriptions which can be fed into Sklearn&rsquo;s vectorizers.
The Python library SpaCy is used for this purpose since it contains pretrained NLP pipelines which simplifies this processing step immensely.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=color:#75715e># Create custom tokenizer</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>tokenizer</span>(desc):
    tokens <span style=color:#f92672>=</span> nlp(desc)
    tokens <span style=color:#f92672>=</span> [word<span style=color:#f92672>.</span>lemma_<span style=color:#f92672>.</span>lower()<span style=color:#f92672>.</span>strip() <span style=color:#66d9ef>for</span> word <span style=color:#f92672>in</span> tokens]
    tokens <span style=color:#f92672>=</span> [word <span style=color:#66d9ef>for</span> word <span style=color:#f92672>in</span> tokens <span style=color:#66d9ef>if</span> word <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> nlp<span style=color:#f92672>.</span>Defaults<span style=color:#f92672>.</span>stop_words <span style=color:#f92672>and</span> <span style=color:#f92672>not</span> set(word)<span style=color:#f92672>.</span>issubset(punctuation)]
    <span style=color:#66d9ef>return</span> tokens
</code></pre></div><p>The function first feeds each description into SpaCy&rsquo;s <em>en_core_web_lg</em> pretrained pipeline, where they are tokenized.
Then, each token is stripped off leading and trailing whitespaces, before being converted to lowercase and lemmatized.
If the token is a punctuation or a stop word, it is removed too.
The result is then a list of tokens that will be used by Sklearn&rsquo;s vectorizers to fit the BoW and TF-IDF models.
The output of the vectorizers will then be a word vector that represents the CVE description that was fed in.</p>
<p>The vectorizers and parameters used are shown below:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>CountVectorizer(tokenizer<span style=color:#f92672>=</span>tokenizer,
                ngram_range<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>),
                max_df<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>,
                min_df<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>,
                max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>,)

TfidfVectorizer(tokenizer<span style=color:#f92672>=</span>tokenizer,
                ngram_range<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>),
                max_df<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>,
                min_df<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>,
                max_features<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>,)
</code></pre></div><h2 id=pca-for-dimensionality-reduction>PCA for Dimensionality Reduction<a hidden class=anchor aria-hidden=true href=#pca-for-dimensionality-reduction>#</a></h2>
<p>As the dimensions of the word vectors are very large due to the large vocabulary accumulated from all the training CVE descriptions, PCA is used reduce the number of dimensions of the word vectors.
However, as including this step increases the computational time by a lot, this step is ommitted in most of the pipelines that I created.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>TruncatedSVD(n_components<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>, n_iter<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</code></pre></div><h2 id=choosing-ml-models>Choosing ML Models<a hidden class=anchor aria-hidden=true href=#choosing-ml-models>#</a></h2>
<p>Since my aim is just to explore which ML model will give a better result, I chose a few simple models to get a basic feel of how well a model can predict the base CVSS V2 scores using their associated CVE descriptions (The models were also chosen with considerations for my potato laptop).</p>
<p>Models chosen were:</p>
<ul>
<li>Linear Regression</li>
<li>KNN</li>
<li>Decision Trees</li>
<li>Gradient Boosting Regression (Xgboost)</li>
</ul>
<p>With the exception of Linear Regression which was fitted on transformed features from both BoW and TF-IDF vectorizers, the other 3 models were only fitted on transformed data from the TF-IDF vectorizer.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>linear_regr <span style=color:#f92672>=</span> LinearRegression()

knn_regr <span style=color:#f92672>=</span> KNeighborsRegressor()

dt_regr <span style=color:#f92672>=</span> DecisionTreeRegressor(min_samples_split<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, min_samples_leaf<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>,)

xgboost_regr <span style=color:#f92672>=</span> GradientBoostingRegressor()
</code></pre></div><h1 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h1>
<p>To score results of the models, Mean Square Error was chosen as the evaluation metric, since I want to penalize models more for larger errors.
As the target base CVSS V2 score only ranges from 0 to 10, the predicted values of the models are further fed into a function that constrains the predicted values to this range (predicted values that are less than 0 to 0, and all predicted values more than 10 to 10).</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>y_pred <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>clip(pipeline<span style=color:#f92672>.</span>predict(X_test), <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>10</span>)
mse <span style=color:#f92672>=</span> mean_squared_error(y_test, y_pred)
</code></pre></div><p>Results are shown in the table below. Do note fine tuning of models have not been done as of the writing of this writeup since it is probably going to take a long time to run and burn up my laptop.</p>
<table>
<thead>
<tr>
<th style=text-align:left>Model</th>
<th style=text-align:left>Vectorizer</th>
<th style=text-align:left>Have PCA</th>
<th style=text-align:left>MSE</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:left>Linear Regression</td>
<td style=text-align:left>BoW</td>
<td style=text-align:left>False</td>
<td style=text-align:left>1.62</td>
</tr>
<tr>
<td style=text-align:left>Linear Regression</td>
<td style=text-align:left>TF-IDF</td>
<td style=text-align:left>False</td>
<td style=text-align:left>1.66</td>
</tr>
<tr>
<td style=text-align:left>Linear Regression</td>
<td style=text-align:left>BoW</td>
<td style=text-align:left>True</td>
<td style=text-align:left>1.59</td>
</tr>
<tr>
<td style=text-align:left>KNN</td>
<td style=text-align:left>TF-IDF</td>
<td style=text-align:left>False</td>
<td style=text-align:left>1.92</td>
</tr>
<tr>
<td style=text-align:left>Decision Trees</td>
<td style=text-align:left>TF-IDF</td>
<td style=text-align:left>False</td>
<td style=text-align:left>2.22</td>
</tr>
<tr>
<td style=text-align:left>Xgboost</td>
<td style=text-align:left>TF-IDF</td>
<td style=text-align:left>False</td>
<td style=text-align:left>1.91</td>
</tr>
</tbody>
</table>
<h1 id=discussion-and-future-work>Discussion and Future Work<a hidden class=anchor aria-hidden=true href=#discussion-and-future-work>#</a></h1>
<p>As can be seen, the BoW vectorizer seem to produce slightly better results than the TF-IDF vectorizer.
Inclusion of the PCA algorithm also provides some slight improvements.
Comparing the different models used, linear regression seem to have produced the best results.</p>
<p>Overall, the best MSE of 1.59 is a good sign, especially since the pipelines are not optimised at all.
This shows that the ML model will on average have an error of about 1.59 when prediciting CVSS scores, which is still acceptable since 1.59 is not too much over the range of 0 to 10.</p>
<p>In the future, work will be done in the following aspects:</p>
<ol>
<li>Constructing a recurrent neural network to compare against the performance of these classic ML algorithms.</li>
<li>Perform more pre-processing on the CVSS scores, since they are not very normally distributed.</li>
<li>Incorporate word vectors and compare performance against the BoW and TF-IDF vectorizers.</li>
<li>Optimising hyperparameters to get an optimal model.</li>
<li>Employing the model for various use cases.</li>
</ol>
<h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1>
<ul>
<li><a href=https://nvd.nist.gov/vuln/data-feeds#>https://nvd.nist.gov/vuln/data-feeds#</a></li>
<li><a href=https://scikit-learn.org/stable/modules/classes.html>https://scikit-learn.org/stable/modules/classes.html</a></li>
<li><a href=https://pandas.pydata.org/docs/>https://pandas.pydata.org/docs/</a></li>
<li><a href=https://spacy.io/usage/linguistic-features>https://spacy.io/usage/linguistic-features</a></li>
<li><a href=https://www.kaggle.com/nkitgupta/text-representations>https://www.kaggle.com/nkitgupta/text-representations</a></li>
<li><a href=https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle>https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle</a></li>
<li><a href=https://www.ibm.com/support/pages/transforming-variable-normality-parametric-statistics>https://www.ibm.com/support/pages/transforming-variable-normality-parametric-statistics</a></li>
<li><a href=https://towardsdatascience.com/working-with-sparse-data-sets-in-pandas-and-sklearn-d26c1cfbe067>https://towardsdatascience.com/working-with-sparse-data-sets-in-pandas-and-sklearn-d26c1cfbe067</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://nusgreyhats.org/tags/writeups/>writeups</a></li>
<li><a href=https://nusgreyhats.org/tags/cyberai/>cyberAI</a></li>
</ul>
<nav class=paginav>
<a class=next href=https://nusgreyhats.org/posts/writeups/introduction-to-digital-forensics/>
<span class=title>Next Page »</span>
<br>
<span>Introduction to Digital Forensics</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Simple ML models to predict CVSS scores on twitter" href="https://twitter.com/intent/tweet/?text=Simple%20ML%20models%20to%20predict%20CVSS%20scores&url=https%3a%2f%2fnusgreyhats.org%2fposts%2fwriteups%2fsimple-ml-models-to-predict-cvss-scores%2f&hashtags=writeups%2ccyberAI"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Simple ML models to predict CVSS scores on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnusgreyhats.org%2fposts%2fwriteups%2fsimple-ml-models-to-predict-cvss-scores%2f&title=Simple%20ML%20models%20to%20predict%20CVSS%20scores&summary=Simple%20ML%20models%20to%20predict%20CVSS%20scores&source=https%3a%2f%2fnusgreyhats.org%2fposts%2fwriteups%2fsimple-ml-models-to-predict-cvss-scores%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Simple ML models to predict CVSS scores on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnusgreyhats.org%2fposts%2fwriteups%2fsimple-ml-models-to-predict-cvss-scores%2f&title=Simple%20ML%20models%20to%20predict%20CVSS%20scores"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Simple ML models to predict CVSS scores on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnusgreyhats.org%2fposts%2fwriteups%2fsimple-ml-models-to-predict-cvss-scores%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Simple ML models to predict CVSS scores on whatsapp" href="https://api.whatsapp.com/send?text=Simple%20ML%20models%20to%20predict%20CVSS%20scores%20-%20https%3a%2f%2fnusgreyhats.org%2fposts%2fwriteups%2fsimple-ml-models-to-predict-cvss-scores%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Simple ML models to predict CVSS scores on telegram" href="https://telegram.me/share/url?text=Simple%20ML%20models%20to%20predict%20CVSS%20scores&url=https%3a%2f%2fnusgreyhats.org%2fposts%2fwriteups%2fsimple-ml-models-to-predict-cvss-scores%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://nusgreyhats.org/>NUS Greyhats</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light'),document.getElementById("logo").src="/greyhats.png"):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'),document.getElementById("logo").src="/greyhats-dark.png")})</script>
</body>
</html>